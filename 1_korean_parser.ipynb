{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-28T21:16:01.523316Z",
     "start_time": "2025-12-28T21:16:01.353264Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "def clean_and_split_sentences(text):\n",
    "    \"\"\"\n",
    "    Очищает текст и разбивает на предложения.\n",
    "    Для корейского используем простую эвристику: .!?… (корейские знаки препинания).\n",
    "    \"\"\"\n",
    "    # 1. Удаляем метаданные (от начала до \"차례\" или \"제 1장\")\n",
    "    start_patterns = [r'차례', r'제 1장', r'제1장']\n",
    "    for pattern in start_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            text = text[match.start():]\n",
    "            break\n",
    "\n",
    "    # 2. Удаляем сноски типа '해리 포터'에 대한 찬사 외\n",
    "    text = re.sub(r'[\\'\\\"].*?외\\.?\\s*\\d*', '', text)\n",
    "\n",
    "    # 3. Разбиваем на предложения\n",
    "    # Корейские завершающие знаки препинания: . ! ? … (многоточие)\n",
    "    sentence_endings = r'[.!?…]+'\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "\n",
    "    # 4. Фильтруем предложения\n",
    "    valid_sentences = []\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if not sent:\n",
    "            continue\n",
    "\n",
    "        # Удаляем номера глав (например: \"제 11장 결투클럽.11\")\n",
    "        sent = re.sub(r'제\\s*\\d+\\s*장\\s*', '', sent)\n",
    "\n",
    "        # Приблизительный подсчет слов (разбиваем по пробелам и корейским разделителям)\n",
    "        # Для корейского лучше считать символы, но для вашей задачи подойдет\n",
    "        words = re.findall(r'[\\w]+', sent)  # Находит корейские и английские слова\n",
    "        if 7 <= len(words) <= 70:\n",
    "            # Проверяем, что предложение начинается с заглавной буквы\n",
    "            if sent and sent[0].isupper():\n",
    "                valid_sentences.append(sent + '.')\n",
    "            else:\n",
    "                # Если нет, делаем заглавной первую букву\n",
    "                valid_sentences.append(sent[0].upper() + sent[1:] + '.')\n",
    "\n",
    "    return valid_sentences\n",
    "\n",
    "def process_book(file_path, output_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    sentences = clean_and_split_sentences(text)\n",
    "\n",
    "    # Сохраняем нужное количество\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for i, sent in enumerate(sentences[:]):\n",
    "            f.write(f\"{sent}\\n\")\n",
    "\n",
    "    print(f\"Сохранено {len(sentences)} предложений в {output_path}\")\n",
    "    return sentences\n",
    "\n",
    "# Обработка двух книг\n",
    "if __name__ == \"__main__\":\n",
    "    # Обработка файлов\n",
    "    book1_sentences = process_book(\"book_korean/processed_books/book_1_utf_8.txt\", \"book_korean/parsed_sentences/korean_sentences_1.txt\")\n",
    "    book2_sentences = process_book(\"book_korean/processed_books/book_2_utf_8.txt\", \"book_korean/parsed_sentences/korean_sentences_2.txt\")\n",
    "    book3_sentences = process_book(\"book_korean/processed_books/book_3_utf_8.txt\", \"book_korean/parsed_sentences/korean_sentences_3.txt\")\n",
    "    book4_sentences = process_book(\"book_korean/processed_books/book_4_utf_8.txt\", \"book_korean/parsed_sentences/korean_sentences_4.txt\")\n",
    "\n",
    "    # Объединяем, если нужно 12к из двух книг\n",
    "    all_sentences = book1_sentences + book2_sentences + book3_sentences + book4_sentences\n",
    "    with open(f\"book_korean/parsed_sentences/korean_sentences_{len(all_sentences[:])}.txt\", 'w', encoding='utf-8') as f:\n",
    "        for sent in all_sentences[:]:\n",
    "            f.write(f\"{sent}\\n\")\n",
    "\n",
    "    print(f\"Итоговый файл: {len(all_sentences[:])} предложений\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранено 3237 предложений в book_korean/parsed_sentences/korean_sentences_1.txt\n",
      "Сохранено 3229 предложений в book_korean/parsed_sentences/korean_sentences_2.txt\n",
      "Сохранено 5288 предложений в book_korean/parsed_sentences/korean_sentences_3.txt\n",
      "Сохранено 8045 предложений в book_korean/parsed_sentences/korean_sentences_4.txt\n",
      "Итоговый файл: 19799 предложений\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T21:27:11.317293Z",
     "start_time": "2025-12-28T21:27:10.975688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def clean_sentence(sent):\n",
    "    \"\"\"\n",
    "    Очистка предложения для TTS\n",
    "    \"\"\"\n",
    "    if not sent or len(sent.strip()) == 0:\n",
    "        return None\n",
    "\n",
    "    # Убираем кавычки любого типа\n",
    "    sent = re.sub(r'[\\\"\\\"\\'\\'「」『』《》]', '', sent)\n",
    "\n",
    "    # Убираем скобки и их содержимое\n",
    "    sent = re.sub(r'\\([^)]*\\)', '', sent)\n",
    "    sent = re.sub(r'\\[[^\\]]*\\]', '', sent)\n",
    "\n",
    "    # Убираем специальные символы\n",
    "    sent = re.sub(r'[#@$%&*_+=|~<>/\\\\]', '', sent)\n",
    "\n",
    "    # Убираем римские цифры в начале (главы)\n",
    "    sent = re.sub(r'^[IVXLCDM]+\\.\\s*', '', sent)\n",
    "\n",
    "    # Убираем лишние пробелы\n",
    "    sent = ' '.join(sent.split())\n",
    "\n",
    "    # Удаляем точки в середине предложения (артефакты номеров страниц)\n",
    "    sent = re.sub(r'(?<=\\w)\\.(?=\\w)', '', sent)\n",
    "\n",
    "    return sent.strip()\n",
    "\n",
    "def clean_and_split_sentences(text):\n",
    "    \"\"\"\n",
    "    Очищает текст и разбивает на предложения.\n",
    "    \"\"\"\n",
    "    # Удаляем метаданные (от начала до \"차례\" или \"제 1장\")\n",
    "    start_patterns = [r'차례', r'제 1장', r'제1장']\n",
    "    for pattern in start_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            text = text[match.start():]\n",
    "            break\n",
    "\n",
    "    # Удаляем сноски и примечания\n",
    "    text = re.sub(r'[\\'\\\"].*?외\\.?\\s*\\d*', '', text)\n",
    "    text = re.sub(r'옮긴이의 말.*', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Заменяем многоточия на обычные точки для разделения\n",
    "    text = text.replace('…', '.')\n",
    "\n",
    "    # Разбиваем на предложения\n",
    "    sentence_endings = r'[.!?]+'\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "\n",
    "    # Фильтруем и очищаем предложения\n",
    "    valid_sentences = []\n",
    "    for sent in sentences:\n",
    "        sent = clean_sentence(sent)\n",
    "        if not sent:\n",
    "            continue\n",
    "            continue\n",
    "\n",
    "        # Удаляем номера глав\n",
    "        sent = re.sub(r'제\\s*\\d+\\s*장\\s*', '', sent)\n",
    "\n",
    "        # Удаляем цифры (по требованию задания - цифры должны быть словами)\n",
    "        # Лучше удалить предложения с цифрами, чем неправильно конвертировать\n",
    "        if re.search(r'\\d+', sent):\n",
    "            continue\n",
    "\n",
    "        # Минимальная длина - 7 символов (не слов!)\n",
    "        if len(sent) < 7:\n",
    "            continue\n",
    "\n",
    "        # Проверяем что начинается с корейской или английской буквы\n",
    "        first_char = sent[0]\n",
    "        is_korean = ('가' <= first_char <= '힣')\n",
    "        is_english = ('A' <= first_char <= 'Z') or ('a' <= first_char <= 'z')\n",
    "\n",
    "        if not (is_korean or is_english):\n",
    "            continue\n",
    "\n",
    "        # Проверяем количество слов (разбиваем по пробелам)\n",
    "        words = sent.split()\n",
    "        if len(words) < 7:  # Минимум 7 слов по заданию\n",
    "            continue\n",
    "\n",
    "        # Максимум 70 слов по заданию\n",
    "        if len(words) > 70:\n",
    "            continue\n",
    "\n",
    "        # Делаем первую букву заглавной если нужно\n",
    "        if sent and sent[0].islower():\n",
    "            sent = sent[0].upper() + sent[1:]\n",
    "\n",
    "        # Добавляем точку в конце если её нет\n",
    "        if not sent.endswith('.'):\n",
    "            sent += '.'\n",
    "\n",
    "        valid_sentences.append(sent)\n",
    "\n",
    "    return valid_sentences\n",
    "\n",
    "def process_all_books(book_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Обрабатывает все книги в папке и сохраняет результат\n",
    "    \"\"\"\n",
    "    # Создаем папки если их нет\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    all_sentences = []\n",
    "    book_files = sorted([f for f in os.listdir(book_folder) if f.endswith('.txt')])\n",
    "\n",
    "    print(f\"Найдено {len(book_files)} книг для обработки\")\n",
    "\n",
    "    for book_file in book_files:\n",
    "        book_path = os.path.join(book_folder, book_file)\n",
    "        print(f\"Обработка: {book_file}\")\n",
    "\n",
    "        try:\n",
    "            with open(book_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "\n",
    "            sentences = clean_and_split_sentences(text)\n",
    "            all_sentences.extend(sentences)\n",
    "\n",
    "            print(f\"  Извлечено: {len(sentences)} предложений\")\n",
    "            print(f\"  Всего накоплено: {len(all_sentences)} предложений\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Ошибка при обработке {book_file}: {e}\")\n",
    "\n",
    "    # Сохраняем все отпарсенные предложения\n",
    "    parsed_path = os.path.join(output_folder, \"korean_parsed.txt\")\n",
    "    with open(parsed_path, 'w', encoding='utf-8') as f:\n",
    "        for sent in all_sentences:\n",
    "            f.write(f\"{sent}\\n\")\n",
    "\n",
    "    print(f\"\\nСохранен файл со всеми предложениями: {parsed_path}\")\n",
    "    print(f\"Всего предложений: {len(all_sentences)}\")\n",
    "\n",
    "    # Сохраняем ровно 12000 предложений (если есть достаточно)\n",
    "    final_path = os.path.join(output_folder, \"korean_final_12000.txt\")\n",
    "    num_to_save = min(12000, len(all_sentences))\n",
    "\n",
    "    with open(final_path, 'w', encoding='utf-8') as f:\n",
    "        for i, sent in enumerate(all_sentences[:num_to_save]):\n",
    "            f.write(f\"{sent}\\n\")\n",
    "\n",
    "    print(f\"Сохранен финальный файл: {final_path}\")\n",
    "    print(f\"Сохранено предложений: {num_to_save}\")\n",
    "\n",
    "    if len(all_sentences) < 12000:\n",
    "        print(f\"ВНИМАНИЕ: Недостаточно предложений! Нужно 12000, есть только {len(all_sentences)}\")\n",
    "        print(\"Добавьте больше книг или ослабьте критерии фильтрации.\")\n",
    "\n",
    "    return all_sentences\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Укажите пути к вашим папкам\n",
    "    book_folder = \"book_korean/processed_books\"\n",
    "    output_folder = \"book_korean/parsed_sentences\"\n",
    "\n",
    "    # Запускаем обработку\n",
    "    all_sentences = process_all_books(book_folder, output_folder)\n",
    "\n",
    "    # Выводим примеры для проверки\n",
    "    print(\"\\nПервые 5 предложений для проверки:\")\n",
    "    for i, sent in enumerate(all_sentences[:5]):\n",
    "        print(f\"{i+1}: {sent}\")\n",
    "\n",
    "    # Статистика по длине\n",
    "    print(\"\\nСтатистика по длине предложений:\")\n",
    "    lengths = [len(s.split()) for s in all_sentences[:1000]]  # Проверяем на первых 1000\n",
    "    if lengths:\n",
    "        print(f\"  Среднее количество слов: {sum(lengths)/len(lengths):.1f}\")\n",
    "        print(f\"  Минимальное количество слов: {min(lengths)}\")\n",
    "        print(f\"  Максимальное количество слов: {max(lengths)}\")\n",
    "        print(f\"  Предложений с более 70 слов: {len([l for l in lengths if l > 70])}\")"
   ],
   "id": "313357cf0ffb7911",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 4 книг для обработки\n",
      "Обработка: book_1_utf_8.txt\n",
      "  Извлечено: 2841 предложений\n",
      "  Всего накоплено: 2841 предложений\n",
      "Обработка: book_2_utf_8.txt\n",
      "  Извлечено: 2971 предложений\n",
      "  Всего накоплено: 5812 предложений\n",
      "Обработка: book_3_utf_8.txt\n",
      "  Извлечено: 5049 предложений\n",
      "  Всего накоплено: 10861 предложений\n",
      "Обработка: book_4_utf_8.txt\n",
      "  Извлечено: 7780 предложений\n",
      "  Всего накоплено: 18641 предложений\n",
      "\n",
      "Сохранен файл со всеми предложениями: book_korean/parsed_sentences\\korean_parsed.txt\n",
      "Всего предложений: 18641\n",
      "Сохранен финальный файл: book_korean/parsed_sentences\\korean_final_12000.txt\n",
      "Сохранено предложений: 12000\n",
      "\n",
      "Первые 5 предложений для проверки:\n",
      "1: 차례차례 무게를 다는 걸 놀란 눈으로 바라보았다.\n",
      "2: 해그리드가 마침내 쬐그마한 황금빛 열쇠 하나를 들어올리며 말했다.\n",
      "3: 해그리드가 가슴에 손을 쭉 펴고, 거드름을 피며 말했다.\n",
      "4: 해그리드는 일단 강아지용 비스킷을 다시 주머니에 쑤셔 넣고, 해리와 함께 그립훅을 따라 홀로 통하는 문 가운데 하나로 향했다.\n",
      "5: 덤블도어 교수가 날 믿고 일을 맡긴 건데 네게 그걸 말하면 난 파면당할 거야.\n",
      "\n",
      "Статистика по длине предложений:\n",
      "  Среднее количество слов: 11.8\n",
      "  Минимальное количество слов: 7\n",
      "  Максимальное количество слов: 39\n",
      "  Предложений с более 70 слов: 0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T21:47:20.867705Z",
     "start_time": "2025-12-28T21:47:20.803540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import hashlib\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "def check_duplicates(file_path, output_report_path=None):\n",
    "    \"\"\"\n",
    "    Проверяет файл на дубликаты предложений\n",
    "    Возвращает статистику и записывает отчет\n",
    "    \"\"\"\n",
    "    print(f\"\\nПроверка файла: {file_path}\")\n",
    "    print(\"Чтение файла...\")\n",
    "\n",
    "    # Читаем все предложения\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    total_lines = len(lines)\n",
    "    print(f\"Всего предложений: {total_lines}\")\n",
    "\n",
    "    # Используем хеши для быстрой проверки\n",
    "    print(\"Вычисление хешей...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Создаем хеши и считаем повторения\n",
    "    hashes = []\n",
    "    for line in lines:\n",
    "        # Нормализуем: удаляем лишние пробелы, приводим к одному регистру\n",
    "        normalized = ' '.join(line.split()).lower()\n",
    "        line_hash = hashlib.md5(normalized.encode('utf-8')).hexdigest()\n",
    "        hashes.append((line_hash, line))\n",
    "\n",
    "    # Находим дубликаты\n",
    "    hash_counter = Counter([h[0] for h in hashes])\n",
    "    duplicate_hashes = {h: count for h, count in hash_counter.items() if count > 1}\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Время проверки: {elapsed:.2f} секунд\")\n",
    "\n",
    "    # Собираем информацию о дубликатах\n",
    "    duplicates_info = {}\n",
    "    for line_hash, line_text in hashes:\n",
    "        if line_hash in duplicate_hashes:\n",
    "            if line_hash not in duplicates_info:\n",
    "                duplicates_info[line_hash] = {\n",
    "                    'count': hash_counter[line_hash],\n",
    "                    'examples': [],\n",
    "                    'first_index': None\n",
    "                }\n",
    "            if len(duplicates_info[line_hash]['examples']) < 3:  # Сохраняем до 3 примеров\n",
    "                duplicates_info[line_hash]['examples'].append(line_text)\n",
    "\n",
    "    # Находим индексы дубликатов\n",
    "    print(\"Поиск позиций дубликатов...\")\n",
    "    for idx, (line_hash, line_text) in enumerate(hashes):\n",
    "        if line_hash in duplicate_hashes:\n",
    "            if duplicates_info[line_hash]['first_index'] is None:\n",
    "                duplicates_info[line_hash]['first_index'] = idx + 1  # +1 для человеческого формата\n",
    "\n",
    "    # Статистика\n",
    "    num_unique = len(set([h[0] for h in hashes]))\n",
    "    num_duplicates = total_lines - num_unique\n",
    "    duplicate_lines = sum([count - 1 for count in hash_counter.values() if count > 1])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"СТАТИСТИКА УНИКАЛЬНОСТИ:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Всего строк: {total_lines}\")\n",
    "    print(f\"Уникальных строк: {num_unique}\")\n",
    "    print(f\"Дублирующих строк: {num_duplicates}\")\n",
    "    print(f\"Всего дубликатов (с повторениями): {duplicate_lines}\")\n",
    "    print(f\"Процент уникальности: {(num_unique/total_lines*100):.2f}%\")\n",
    "\n",
    "    if duplicate_hashes:\n",
    "        print(f\"\\nНайдено {len(duplicate_hashes)} уникальных предложений с дубликатами:\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        for i, (h, info) in enumerate(list(duplicates_info.items())[:10]):  # Показываем первые 10\n",
    "            print(f\"\\nДубликат #{i+1}:\")\n",
    "            print(f\"  Количество повторений: {info['count']}\")\n",
    "            print(f\"  Первое вхождение: строка {info['first_index']}\")\n",
    "            print(f\"  Пример текста: \\\"{info['examples'][0][:80]}...\\\"\")\n",
    "\n",
    "        if len(duplicate_hashes) > 10:\n",
    "            print(f\"\\n... и еще {len(duplicate_hashes) - 10} дубликатов\")\n",
    "\n",
    "    # Создаем отчет если указан путь\n",
    "    if output_report_path:\n",
    "        create_report(output_report_path, total_lines, num_unique, duplicates_info, lines, hashes)\n",
    "\n",
    "    return {\n",
    "        'total': total_lines,\n",
    "        'unique': num_unique,\n",
    "        'duplicate_count': len(duplicate_hashes),\n",
    "        'duplicate_lines': duplicate_lines,\n",
    "        'duplicates': duplicates_info\n",
    "    }\n",
    "\n",
    "def create_report(report_path, total, unique, duplicates_info, original_lines, hashes):\n",
    "    \"\"\"Создает подробный отчет о дубликатах\"\"\"\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"ОТЧЕТ О ПРОВЕРКЕ УНИКАЛЬНОСТИ\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "\n",
    "        f.write(f\"Всего предложений: {total}\\n\")\n",
    "        f.write(f\"Уникальных предложений: {unique}\\n\")\n",
    "        f.write(f\"Дублирующих предложений: {total - unique}\\n\")\n",
    "        f.write(f\"Процент уникальности: {(unique/total*100):.2f}%\\n\\n\")\n",
    "\n",
    "        if duplicates_info:\n",
    "            f.write(\"ПОДРОБНАЯ ИНФОРМАЦИЯ О ДУБЛИКАТАХ:\\n\")\n",
    "            f.write(\"-\"*60 + \"\\n\\n\")\n",
    "\n",
    "            for i, (h, info) in enumerate(duplicates_info.items()):\n",
    "                f.write(f\"ДУБЛИКАТ #{i+1}\\n\")\n",
    "                f.write(f\"Количество повторений: {info['count']}\\n\")\n",
    "                f.write(f\"Первое вхождение: строка {info['first_index']}\\n\")\n",
    "                f.write(f\"Текст: {info['examples'][0]}\\n\")\n",
    "\n",
    "                # Находим все позиции этого дубликата\n",
    "                positions = []\n",
    "                for idx, (line_hash, _) in enumerate(hashes):\n",
    "                    if line_hash == h:\n",
    "                        positions.append(idx + 1)\n",
    "\n",
    "                f.write(f\"Позиции в файле: {positions}\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\\n\")\n",
    "\n",
    "            # Создаем список строк для удаления (оставляем только первое вхождение)\n",
    "            f.write(\"РЕКОМЕНДАЦИИ ДЛЯ ОЧИСТКИ:\\n\")\n",
    "            f.write(\"-\"*60 + \"\\n\")\n",
    "            f.write(\"Строки для удаления (все, кроме первого вхождения):\\n\")\n",
    "\n",
    "            lines_to_remove = []\n",
    "            for h, info in duplicates_info.items():\n",
    "                # Находим все позиции этого хеша\n",
    "                positions = []\n",
    "                for idx, (line_hash, _) in enumerate(hashes):\n",
    "                    if line_hash == h:\n",
    "                        positions.append(idx)\n",
    "\n",
    "                # Оставляем первую позицию, остальные помечаем для удаления\n",
    "                if len(positions) > 1:\n",
    "                    lines_to_remove.extend(positions[1:])\n",
    "\n",
    "            lines_to_remove.sort()\n",
    "            for idx in lines_to_remove[:50]:  # Показываем первые 50\n",
    "                f.write(f\"Строка {idx + 1}: {original_lines[idx][:100]}...\\n\")\n",
    "\n",
    "            if len(lines_to_remove) > 50:\n",
    "                f.write(f\"... и еще {len(lines_to_remove) - 50} строк\\n\")\n",
    "\n",
    "            f.write(f\"\\nВсего строк для удаления: {len(lines_to_remove)}\\n\")\n",
    "\n",
    "        else:\n",
    "            f.write(\"ДУБЛИКАТЫ НЕ НАЙДЕНЫ. Файл полностью уникален.\\n\")\n",
    "\n",
    "    print(f\"\\nПолный отчет сохранен в: {report_path}\")\n",
    "\n",
    "def remove_duplicates(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Создает новый файл без дубликатов, сохраняя порядок\n",
    "    \"\"\"\n",
    "    print(f\"\\nСоздание файла без дубликатов...\")\n",
    "\n",
    "    seen_hashes = set()\n",
    "    unique_lines = []\n",
    "    duplicates_removed = 0\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            text = line.strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # Нормализуем и хешируем\n",
    "            normalized = ' '.join(text.split()).lower()\n",
    "            line_hash = hashlib.md5(normalized.encode('utf-8')).hexdigest()\n",
    "\n",
    "            if line_hash not in seen_hashes:\n",
    "                seen_hashes.add(line_hash)\n",
    "                unique_lines.append(text)\n",
    "            else:\n",
    "                duplicates_removed += 1\n",
    "\n",
    "    # Сохраняем уникальные строки\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for line in unique_lines:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "    print(f\"Удалено дубликатов: {duplicates_removed}\")\n",
    "    print(f\"Сохранено уникальных строк: {len(unique_lines)}\")\n",
    "    print(f\"Новый файл: {output_path}\")\n",
    "\n",
    "    return len(unique_lines)\n",
    "\n",
    "def compare_two_files(file1_path, file2_path):\n",
    "    \"\"\"\n",
    "    Сравнивает два файла на общие предложения\n",
    "    \"\"\"\n",
    "    print(f\"\\nСравнение файлов:\")\n",
    "    print(f\"  Файл 1: {file1_path}\")\n",
    "    print(f\"  Файл 2: {file2_path}\")\n",
    "\n",
    "    # Читаем оба файла\n",
    "    with open(file1_path, 'r', encoding='utf-8') as f:\n",
    "        lines1 = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    with open(file2_path, 'r', encoding='utf-8') as f:\n",
    "        lines2 = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    print(f\"  Предложений в файле 1: {len(lines1)}\")\n",
    "    print(f\"  Предложений в файле 2: {len(lines2)}\")\n",
    "\n",
    "    # Создаем множества хешей\n",
    "    hashes1 = set()\n",
    "    for line in lines1:\n",
    "        normalized = ' '.join(line.split()).lower()\n",
    "        line_hash = hashlib.md5(normalized.encode('utf-8')).hexdigest()\n",
    "        hashes1.add(line_hash)\n",
    "\n",
    "    hashes2 = set()\n",
    "    common_hashes = set()\n",
    "    for line in lines2:\n",
    "        normalized = ' '.join(line.split()).lower()\n",
    "        line_hash = hashlib.md5(normalized.encode('utf-8')).hexdigest()\n",
    "        hashes2.add(line_hash)\n",
    "        if line_hash in hashes1:\n",
    "            common_hashes.add(line_hash)\n",
    "\n",
    "    # Статистика\n",
    "    common_count = len(common_hashes)\n",
    "    unique_to_file1 = len(hashes1) - common_count\n",
    "    unique_to_file2 = len(hashes2) - common_count\n",
    "\n",
    "    print(\"\\nРЕЗУЛЬТАТЫ СРАВНЕНИЯ:\")\n",
    "    print(f\"Общих предложений: {common_count}\")\n",
    "    print(f\"Уникальных для файла 1: {unique_to_file1}\")\n",
    "    print(f\"Уникальных для файла 2: {unique_to_file2}\")\n",
    "\n",
    "    if common_hashes:\n",
    "        print(f\"\\nПримеры общих предложений:\")\n",
    "        # Находим примеры общих предложений\n",
    "        common_examples = []\n",
    "        for line in lines1:\n",
    "            normalized = ' '.join(line.split()).lower()\n",
    "            line_hash = hashlib.md5(normalized.encode('utf-8')).hexdigest()\n",
    "            if line_hash in common_hashes and len(common_examples) < 5:\n",
    "                common_examples.append(line)\n",
    "\n",
    "        for i, example in enumerate(common_examples):\n",
    "            print(f\"{i+1}. {example[:80]}...\")\n",
    "\n",
    "    return common_count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Укажите пути к вашим файлам\n",
    "    # parsed_file = \"book_korean/parsed_sentences/korean_parsed.txt\"\n",
    "    final_file = \"book_korean/parsed_sentences/korean_final_12000_unique.txt\"\n",
    "\n",
    "    # 1. Проверяем файл со всеми предложениями\n",
    "    # stats1 = check_duplicates(\n",
    "    #     parsed_file,\n",
    "    #     \"book_korean/parsed_sentences/duplicates_report_full.txt\"\n",
    "    # )\n",
    "\n",
    "    # 2. Проверяем финальный файл (12000 предложений)\n",
    "    stats2 = check_duplicates(\n",
    "        final_file,\n",
    "        \"book_korean/parsed_sentences/duplicates_report_final.txt\"\n",
    "    )\n",
    "\n",
    "    # 3. Сравниваем два файла\n",
    "    # common = compare_two_files(parsed_file, final_file)\n",
    "    #\n",
    "    # # 4. Создаем очищенные версии если есть дубликаты\n",
    "    # if stats1['duplicate_lines'] > 0:\n",
    "    #     remove_duplicates(\n",
    "    #         parsed_file,\n",
    "    #         \"book_korean/parsed_sentences/korean_parsed_unique.txt\"\n",
    "    #     )\n",
    "\n",
    "    # if stats2['duplicate_lines'] > 0:\n",
    "    #     remove_duplicates(\n",
    "    #         final_file,\n",
    "    #         \"book_korean/parsed_sentences/korean_final_12000_unique.txt\"\n",
    "    #     )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ИТОГОВАЯ СТАТИСТИКА:\")\n",
    "    print(\"=\"*60)\n",
    "    # print(f\"Файл {parsed_file}:\")\n",
    "    # print(f\"  Уникальность: {stats1['unique']}/{stats1['total']} ({(stats1['unique']/stats1['total']*100):.2f}%)\")\n",
    "\n",
    "    print(f\"\\nФайл {final_file}:\")\n",
    "    print(f\"  Уникальность: {stats2['unique']}/{stats2['total']} ({(stats2['unique']/stats2['total']*100):.2f}%)\")\n",
    "\n",
    "    # if common > 0:\n",
    "    #     print(f\"\\nВНИМАНИЕ: Найдено {common} общих предложений между файлами!\")\n",
    "    #     print(\"Это нормально, так как final_file является подмножеством parsed_file\")"
   ],
   "id": "8469d02cd77aa0a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Проверка файла: book_korean/parsed_sentences/korean_final_12000_unique.txt\n",
      "Чтение файла...\n",
      "Всего предложений: 12000\n",
      "Вычисление хешей...\n",
      "Время проверки: 0.03 секунд\n",
      "Поиск позиций дубликатов...\n",
      "\n",
      "==================================================\n",
      "СТАТИСТИКА УНИКАЛЬНОСТИ:\n",
      "==================================================\n",
      "Всего строк: 12000\n",
      "Уникальных строк: 12000\n",
      "Дублирующих строк: 0\n",
      "Всего дубликатов (с повторениями): 0\n",
      "Процент уникальности: 100.00%\n",
      "\n",
      "Полный отчет сохранен в: book_korean/parsed_sentences/duplicates_report_final.txt\n",
      "\n",
      "============================================================\n",
      "ИТОГОВАЯ СТАТИСТИКА:\n",
      "============================================================\n",
      "\n",
      "Файл book_korean/parsed_sentences/korean_final_12000_unique.txt:\n",
      "  Уникальность: 12000/12000 (100.00%)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "589572d28747c757"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
